{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Exercise: Starbucks\n",
    "<br>\n",
    "\n",
    "<img src=\"https://opj.ca/wp-content/uploads/2018/02/New-Starbucks-Logo-1200x969.jpg\" width=\"200\" height=\"200\">\n",
    "<br>\n",
    "<br>\n",
    " \n",
    "#### Background Information\n",
    "\n",
    "The dataset you will be provided in this portfolio exercise was originally used as a take-home assignment provided by Starbucks for their job candidates. The data for this exercise consists of about 120,000 data points split in a 2:1 ratio among training and test files. In the experiment simulated by the data, an advertising promotion was tested to see if it would bring more customers to purchase a specific product priced at $10. Since it costs the company 0.15 to send out each promotion, it would be best to limit that promotion only to those that are most receptive to the promotion. Each data point includes one column indicating whether or not an individual was sent a promotion for the product, and one column indicating whether or not that individual eventually purchased that product. Each individual also has seven additional features associated with them, which are provided abstractly as V1-V7.\n",
    "\n",
    "#### Optimization Strategy\n",
    "\n",
    "Your task is to use the training data to understand what patterns in V1-V7 to indicate that a promotion should be provided to a user. Specifically, your goal is to maximize the following metrics:\n",
    "\n",
    "* **Incremental Response Rate (IRR)** \n",
    "\n",
    "IRR depicts how many more customers purchased the product with the promotion, as compared to if they didn't receive the promotion. Mathematically, it's the ratio of the number of purchasers in the promotion group to the total number of customers in the purchasers group (_treatment_) minus the ratio of the number of purchasers in the non-promotional group to the total number of customers in the non-promotional group (_control_).\n",
    "\n",
    "$$ IRR = \\frac{purch_{treat}}{cust_{treat}} - \\frac{purch_{ctrl}}{cust_{ctrl}} $$\n",
    "\n",
    "\n",
    "* **Net Incremental Revenue (NIR)**\n",
    "\n",
    "NIR depicts how much is made (or lost) by sending out the promotion. Mathematically, this is 10 times the total number of purchasers that received the promotion minus 0.15 times the number of promotions sent out, minus 10 times the number of purchasers who were not given the promotion.\n",
    "\n",
    "$$ NIR = (10\\cdot purch_{treat} - 0.15 \\cdot cust_{treat}) - 10 \\cdot purch_{ctrl}$$\n",
    "\n",
    "For a full description of what Starbucks provides to candidates see the [instructions available here](https://drive.google.com/open?id=18klca9Sef1Rs6q8DW4l7o349r8B70qXM).\n",
    "\n",
    "Below you can find the training data provided.  Explore the data and different optimization strategies.\n",
    "\n",
    "#### How To Test Your Strategy?\n",
    "\n",
    "When you feel like you have an optimization strategy, complete the `promotion_strategy` function to pass to the `test_results` function.  \n",
    "From past data, we know there are four possible outomes:\n",
    "\n",
    "Table of actual promotion vs. predicted promotion customers:  \n",
    "\n",
    "<table>\n",
    "<tr><th></th><th colspan = '2'>Actual</th></tr>\n",
    "<tr><th>Predicted</th><th>Yes</th><th>No</th></tr>\n",
    "<tr><th>Yes</th><td>I</td><td>II</td></tr>\n",
    "<tr><th>No</th><td>III</td><td>IV</td></tr>\n",
    "</table>\n",
    "\n",
    "The metrics are only being compared for the individuals we predict should obtain the promotion â€“ that is, quadrants I and II.  Since the first set of individuals that receive the promotion (in the training set) receive it randomly, we can expect that quadrants I and II will have approximately equivalent participants.  \n",
    "\n",
    "Comparing quadrant I to II then gives an idea of how well your promotion strategy will work in the future. \n",
    "\n",
    "Get started by reading in the data below.  See how each variable or combination of variables along with a promotion influences the chance of purchasing.  When you feel like you have a strategy for who should receive a promotion, test your strategy against the test dataset used in the final `test_results` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-a0cf81008cd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m \u001b[0;31m# we can try this as a resampling technique to address class imbalance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "# load in packages\n",
    "from itertools import combinations\n",
    "\n",
    "from test_results import test_results, score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE # we can try this as a resampling technique to address class imbalance\n",
    "\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install -U imbalanced-learn\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.443518</td>\n",
       "      <td>-1.165083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.159350</td>\n",
       "      <td>-0.645617</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.431659</td>\n",
       "      <td>0.133583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.588914</td>\n",
       "      <td>-0.212728</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.044332</td>\n",
       "      <td>-0.385883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Promotion  purchase  V1         V2        V3  V4  V5  V6  V7\n",
       "0   1        No         0   2  30.443518 -1.165083   1   1   3   2\n",
       "1   3        No         0   3  32.159350 -0.645617   2   3   2   2\n",
       "2   4        No         0   2  30.431659  0.133583   1   1   4   2\n",
       "3   5        No         0   0  26.588914 -0.212728   2   1   4   2\n",
       "4   8       Yes         0   3  28.044332 -0.385883   1   1   2   2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the data\n",
    "train_data = pd.read_csv('./training.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cells for you to work and document as necessary - \n",
    "# definitely feel free to add more cells as you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84534"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    83494\n",
       "1     1040\n",
       "Name: purchase, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['purchase'].value_counts() # note the huge imbalance, which will affect neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the control and treatment groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    42364\n",
       "No     42170\n",
       "Name: Promotion, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Promotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is probably the place to bring in the historgrams of V1-V7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f51143b22b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f511439ac18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['V4'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the 'purchase' var significantly different in the two groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group = train_data[train_data['Promotion']=='No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_group = train_data[train_data['Promotion']=='Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    41643\n",
       "1      721\n",
       "Name: purchase, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_group['purchase'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cont = len(control_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_treat = len(treatment_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "purch_control = (len(control_group[control_group['purchase']==1])) / n_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007564619397676073"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purch_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "purch_treat = (len(treatment_group[treatment_group['purchase']==1])) / n_treat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017019167217448776"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purch_treat # suggests a really unoptimised promotion strategy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test invariant metric - I can see they're basically the same but let's practice some of the course material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = np.sqrt(p * (1-p)* n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ((n_cont + 0.5) - p*n_obs) / sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stat = 2*sp.stats.norm.cdf(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50681406854190458"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_stat # absolutely nowhere near rejecting the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Var metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = train_data['purchase'].mean() # This is not drawn from a priori assumptions but from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE = np.sqrt(P * (1-P) *( 1/n_cont + 1/n_treat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = (purch_treat - purch_control) / SE # again not done a priori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_var = 1 - sp.stats.norm.cdf(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_var # These are two very distinct distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was the treatment successful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental Response Rate  - change in uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009454547819772702"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purch_treat - purch_control # so the promotion increased the uptake proportion by almost a 1 percent\n",
    "# Remember that purch_treat and purch_control are already divided by the group size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net Incremental Revenue - change in $$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_treat_purchasers = len(treatment_group[treatment_group['purchase']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_treat_purchasers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_control_purchasers = len(control_group[control_group['purchase']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_cost = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIR = ((n_treat_purchasers*10) - (n_treat_purchasers*treatment_cost)) - (n_control_purchasers*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3911.8500000000004"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NIR # Additional revenue of $3911. Factor in costs from employee time spent on designing promo, making materials,\n",
    "# setting up data collection framework, governance etc, this probably isn't worth it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at relationships between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_data = train_data.drop(['ID', 'Promotion'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>purchase</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004906</td>\n",
       "      <td>-0.001694</td>\n",
       "      <td>-0.006395</td>\n",
       "      <td>0.032245</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>-0.001538</td>\n",
       "      <td>-0.001117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>-0.004906</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>-0.003672</td>\n",
       "      <td>0.001436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>-0.001694</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>-0.004156</td>\n",
       "      <td>-0.002293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>-0.006395</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>-0.001736</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>0.032245</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>-0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>0.007814</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>-0.001046</td>\n",
       "      <td>-0.001736</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>0.003571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>-0.001538</td>\n",
       "      <td>-0.003672</td>\n",
       "      <td>-0.004156</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>-0.001117</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>-0.002293</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>-0.003176</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>-0.001398</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          purchase        V1        V2        V3        V4        V5  \\\n",
       "purchase  1.000000 -0.004906 -0.001694 -0.006395  0.032245  0.007814   \n",
       "V1       -0.004906  1.000000 -0.001564  0.003995  0.002568 -0.001196   \n",
       "V2       -0.001694 -0.001564  1.000000  0.001165  0.002848 -0.001046   \n",
       "V3       -0.006395  0.003995  0.001165  1.000000  0.003753 -0.001736   \n",
       "V4        0.032245  0.002568  0.002848  0.003753  1.000000  0.003161   \n",
       "V5        0.007814 -0.001196 -0.001046 -0.001736  0.003161  1.000000   \n",
       "V6       -0.001538 -0.003672 -0.004156  0.004464  0.002552 -0.003043   \n",
       "V7       -0.001117  0.001436 -0.002293  0.001135 -0.003176  0.003571   \n",
       "\n",
       "                V6        V7  \n",
       "purchase -0.001538 -0.001117  \n",
       "V1       -0.003672  0.001436  \n",
       "V2       -0.004156 -0.002293  \n",
       "V3        0.004464  0.001135  \n",
       "V4        0.002552 -0.003176  \n",
       "V5       -0.003043  0.003571  \n",
       "V6        1.000000 -0.001398  \n",
       "V7       -0.001398  1.000000  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = correlation_data.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a regression tell us in terms of effects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_data['purchase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = sm.OLS(Y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = reg_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               purchase   R-squared:                       0.013\n",
      "Model:                            OLS   Adj. R-squared:                  0.013\n",
      "Method:                 Least Squares   F-statistic:                     164.8\n",
      "Date:                Mon, 27 May 2024   Prob (F-statistic):          4.11e-243\n",
      "Time:                        16:42:51   Log-Likelihood:                 66512.\n",
      "No. Observations:               84534   AIC:                        -1.330e+05\n",
      "Df Residuals:                   84527   BIC:                        -1.329e+05\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "V1            -0.0006      0.000     -1.466      0.143      -0.001       0.000\n",
      "V2         -3.786e-05   5.59e-05     -0.677      0.499      -0.000    7.18e-05\n",
      "V3            -0.0007      0.000     -1.883      0.060      -0.001    2.92e-05\n",
      "V4             0.0076      0.001     10.259      0.000       0.006       0.009\n",
      "V5             0.0010      0.000      2.363      0.018       0.000       0.002\n",
      "V6            -0.0002      0.000     -0.472      0.637      -0.001       0.000\n",
      "V7            -0.0002      0.001     -0.318      0.750      -0.002       0.001\n",
      "==============================================================================\n",
      "Omnibus:                   119968.983   Durbin-Watson:                   2.000\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         21504815.887\n",
      "Skew:                           8.833   Prob(JB):                         0.00\n",
      "Kurtosis:                      79.114   Cond. No.                         66.4\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep for ANN and creating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test data - do we actually need to do this, given that we have a test data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16907"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16907"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the normalisation steps here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled= scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us an idea of which variables might be driving a consumer's decision to buy the product or not\n",
    "# We can use this information to refine the neural net potentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will want to compare predicted with observed - test data has observations for promo and purchase. Will also want to compare IRR and NIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(7, input_dim=7, activation = 'relu'))\n",
    "model.add(Dense(5, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54101 samples, validate on 13526 samples\n",
      "Epoch 1/20\n",
      "54101/54101 [==============================] - 3s 56us/step - loss: 0.1320 - acc: 0.9816 - val_loss: 0.0692 - val_acc: 0.9884\n",
      "Epoch 2/20\n",
      "54101/54101 [==============================] - 3s 56us/step - loss: 0.0717 - acc: 0.9876 - val_loss: 0.0663 - val_acc: 0.9884\n",
      "Epoch 3/20\n",
      "54101/54101 [==============================] - 3s 56us/step - loss: 0.0694 - acc: 0.9876 - val_loss: 0.0649 - val_acc: 0.9884\n",
      "Epoch 4/20\n",
      "54101/54101 [==============================] - 3s 59us/step - loss: 0.0679 - acc: 0.9876 - val_loss: 0.0638 - val_acc: 0.9884\n",
      "Epoch 5/20\n",
      "54101/54101 [==============================] - 3s 55us/step - loss: 0.0672 - acc: 0.9876 - val_loss: 0.0636 - val_acc: 0.9884\n",
      "Epoch 6/20\n",
      "54101/54101 [==============================] - 3s 57us/step - loss: 0.0669 - acc: 0.9876 - val_loss: 0.0634 - val_acc: 0.9884\n",
      "Epoch 7/20\n",
      "54101/54101 [==============================] - 3s 58us/step - loss: 0.0665 - acc: 0.9876 - val_loss: 0.0634 - val_acc: 0.9884\n",
      "Epoch 8/20\n",
      "54101/54101 [==============================] - 3s 56us/step - loss: 0.0666 - acc: 0.9876 - val_loss: 0.0632 - val_acc: 0.9884\n",
      "Epoch 9/20\n",
      "54101/54101 [==============================] - 3s 55us/step - loss: 0.0665 - acc: 0.9876 - val_loss: 0.0633 - val_acc: 0.9884\n",
      "Epoch 10/20\n",
      "54101/54101 [==============================] - 3s 54us/step - loss: 0.0664 - acc: 0.9876 - val_loss: 0.0634 - val_acc: 0.9884\n",
      "Epoch 11/20\n",
      "54101/54101 [==============================] - 3s 52us/step - loss: 0.0664 - acc: 0.9876 - val_loss: 0.0634 - val_acc: 0.9884\n",
      "Epoch 12/20\n",
      "54101/54101 [==============================] - 3s 53us/step - loss: 0.0664 - acc: 0.9876 - val_loss: 0.0632 - val_acc: 0.9884\n",
      "Epoch 13/20\n",
      "54101/54101 [==============================] - 3s 54us/step - loss: 0.0664 - acc: 0.9876 - val_loss: 0.0638 - val_acc: 0.9884\n",
      "Epoch 14/20\n",
      "54101/54101 [==============================] - 3s 53us/step - loss: 0.0664 - acc: 0.9876 - val_loss: 0.0635 - val_acc: 0.9884\n",
      "Epoch 15/20\n",
      "54101/54101 [==============================] - 3s 53us/step - loss: 0.0664 - acc: 0.9876 - val_loss: 0.0632 - val_acc: 0.9884\n",
      "Epoch 16/20\n",
      "54101/54101 [==============================] - 3s 54us/step - loss: 0.0664 - acc: 0.9876 - val_loss: 0.0646 - val_acc: 0.9884\n",
      "Epoch 17/20\n",
      "54101/54101 [==============================] - 3s 54us/step - loss: 0.0663 - acc: 0.9876 - val_loss: 0.0632 - val_acc: 0.9884\n",
      "Epoch 18/20\n",
      "54101/54101 [==============================] - 3s 59us/step - loss: 0.0663 - acc: 0.9876 - val_loss: 0.0632 - val_acc: 0.9884\n",
      "Epoch 19/20\n",
      "54101/54101 [==============================] - 3s 54us/step - loss: 0.0663 - acc: 0.9876 - val_loss: 0.0633 - val_acc: 0.9884\n",
      "Epoch 20/20\n",
      "54101/54101 [==============================] - 3s 54us/step - loss: 0.0663 - acc: 0.9876 - val_loss: 0.0632 - val_acc: 0.9884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f50bc361240>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_scaled, y_train, epochs = 20, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can see that val accuracy hasn't really changed at all, so super early convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test_scaled)\n",
    "preds = np.round(preds).astype(int)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16695     0]\n",
      " [  212     0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, preds)) # So this has predicted everything as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try one of the weighting techniques for addressing the class imbalance - if this doesn't work can try resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16907"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16907"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16907"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model.add(Dense(50, input_dim=7, activation = 'relu'))\n",
    "weighted_model.add(Dense(35, activation = 'relu'))\n",
    "weighted_model.add(Dense(35, activation = 'relu'))\n",
    "weighted_model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54101 samples, validate on 13526 samples\n",
      "Epoch 1/30\n",
      "54101/54101 [==============================] - 4s 71us/step - loss: 0.1860 - acc: 0.6961 - val_loss: 0.1829 - val_acc: 0.4004\n",
      "Epoch 2/30\n",
      "54101/54101 [==============================] - 3s 64us/step - loss: 0.1845 - acc: 0.6244 - val_loss: 0.1844 - val_acc: 0.6793\n",
      "Epoch 3/30\n",
      "54101/54101 [==============================] - 4s 67us/step - loss: 0.1844 - acc: 0.6285 - val_loss: 0.1829 - val_acc: 0.9728\n",
      "Epoch 4/30\n",
      "54101/54101 [==============================] - 4s 68us/step - loss: 0.1842 - acc: 0.6678 - val_loss: 0.1837 - val_acc: 0.6825\n",
      "Epoch 5/30\n",
      "54101/54101 [==============================] - 4s 66us/step - loss: 0.1840 - acc: 0.6867 - val_loss: 0.1820 - val_acc: 0.6510\n",
      "Epoch 6/30\n",
      "54101/54101 [==============================] - 4s 67us/step - loss: 0.1838 - acc: 0.7143 - val_loss: 0.1841 - val_acc: 0.3902\n",
      "Epoch 7/30\n",
      "54101/54101 [==============================] - 3s 63us/step - loss: 0.1835 - acc: 0.7192 - val_loss: 0.1844 - val_acc: 0.4020\n",
      "Epoch 8/30\n",
      "54101/54101 [==============================] - 4s 66us/step - loss: 0.1836 - acc: 0.6861 - val_loss: 0.1845 - val_acc: 0.6086\n",
      "Epoch 9/30\n",
      "54101/54101 [==============================] - 4s 65us/step - loss: 0.1835 - acc: 0.6076 - val_loss: 0.1838 - val_acc: 0.9463\n",
      "Epoch 10/30\n",
      "54101/54101 [==============================] - 4s 68us/step - loss: 0.1834 - acc: 0.7320 - val_loss: 0.1819 - val_acc: 0.7250\n",
      "Epoch 11/30\n",
      "54101/54101 [==============================] - 4s 69us/step - loss: 0.1832 - acc: 0.6753 - val_loss: 0.1819 - val_acc: 0.7688\n",
      "Epoch 12/30\n",
      "54101/54101 [==============================] - 3s 65us/step - loss: 0.1830 - acc: 0.7609 - val_loss: 0.1815 - val_acc: 0.7713\n",
      "Epoch 13/30\n",
      "54101/54101 [==============================] - 3s 64us/step - loss: 0.1830 - acc: 0.7044 - val_loss: 0.1833 - val_acc: 0.5750\n",
      "Epoch 14/30\n",
      "54101/54101 [==============================] - 3s 63us/step - loss: 0.1830 - acc: 0.6714 - val_loss: 0.1820 - val_acc: 0.7747\n",
      "Epoch 15/30\n",
      "54101/54101 [==============================] - 4s 68us/step - loss: 0.1826 - acc: 0.7166 - val_loss: 0.1859 - val_acc: 0.5041\n",
      "Epoch 16/30\n",
      "54101/54101 [==============================] - 4s 65us/step - loss: 0.1827 - acc: 0.6839 - val_loss: 0.1829 - val_acc: 0.7415\n",
      "Epoch 17/30\n",
      "54101/54101 [==============================] - 4s 68us/step - loss: 0.1822 - acc: 0.6870 - val_loss: 0.1814 - val_acc: 0.7693\n",
      "Epoch 18/30\n",
      "54101/54101 [==============================] - 4s 66us/step - loss: 0.1817 - acc: 0.7023 - val_loss: 0.1811 - val_acc: 0.6198\n",
      "Epoch 19/30\n",
      "54101/54101 [==============================] - 4s 68us/step - loss: 0.1815 - acc: 0.7101 - val_loss: 0.1887 - val_acc: 0.6502\n",
      "Epoch 20/30\n",
      "54101/54101 [==============================] - 4s 66us/step - loss: 0.1814 - acc: 0.6902 - val_loss: 0.1843 - val_acc: 0.5928\n",
      "Epoch 21/30\n",
      "54101/54101 [==============================] - 4s 68us/step - loss: 0.1810 - acc: 0.6790 - val_loss: 0.1816 - val_acc: 0.6636\n",
      "Epoch 22/30\n",
      "54101/54101 [==============================] - 4s 65us/step - loss: 0.1808 - acc: 0.6912 - val_loss: 0.1818 - val_acc: 0.7210\n",
      "Epoch 23/30\n",
      "54101/54101 [==============================] - 4s 68us/step - loss: 0.1802 - acc: 0.7041 - val_loss: 0.1846 - val_acc: 0.4810\n",
      "Epoch 24/30\n",
      "54101/54101 [==============================] - 4s 65us/step - loss: 0.1808 - acc: 0.6310 - val_loss: 0.1849 - val_acc: 0.7052\n",
      "Epoch 25/30\n",
      "54101/54101 [==============================] - 4s 65us/step - loss: 0.1793 - acc: 0.6661 - val_loss: 0.1843 - val_acc: 0.8061\n",
      "Epoch 26/30\n",
      "54101/54101 [==============================] - 3s 64us/step - loss: 0.1790 - acc: 0.6645 - val_loss: 0.1876 - val_acc: 0.7048\n",
      "Epoch 27/30\n",
      "54101/54101 [==============================] - 3s 63us/step - loss: 0.1790 - acc: 0.6592 - val_loss: 0.1824 - val_acc: 0.6841\n",
      "Epoch 28/30\n",
      "54101/54101 [==============================] - 4s 66us/step - loss: 0.1778 - acc: 0.6483 - val_loss: 0.1927 - val_acc: 0.7823\n",
      "Epoch 29/30\n",
      "54101/54101 [==============================] - 4s 69us/step - loss: 0.1778 - acc: 0.6589 - val_loss: 0.1878 - val_acc: 0.6123\n",
      "Epoch 30/30\n",
      "54101/54101 [==============================] - 4s 71us/step - loss: 0.1776 - acc: 0.6696 - val_loss: 0.1845 - val_acc: 0.5852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f503ff6b7b8>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_grid = {0: 0.15, 1: 10}\n",
    "weighted_model.fit(x_train_scaled, y_train, epochs=30, batch_size=64, validation_split=0.2, class_weight=weights_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_preds = model.predict(x_test_scaled)\n",
    "weighted_preds = np.round(weighted_preds).astype(int)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16695     0]\n",
      " [  212     0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, weighted_preds)) # So this has predicted everything as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(Dense(50, input_dim=7, activation = 'relu'))\n",
    "model2.add(Dense(35, activation = 'relu'))\n",
    "model2.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(x_train, y_train, epochs = 20, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = model.predict(x_test)\n",
    "preds2 = np.round(preds2).astype(int)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, preds2)) # So this has predicted everything as positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotion_strategy(df):\n",
    "    '''\n",
    "    INPUT \n",
    "    df - a dataframe with *only* the columns V1 - V7 (same as train_data)\n",
    "\n",
    "    OUTPUT\n",
    "    promotion_df - np.array with the values\n",
    "                   'Yes' or 'No' related to whether or not an \n",
    "                   individual should recieve a promotion \n",
    "                   should be the length of df.shape[0]\n",
    "                \n",
    "    Ex:\n",
    "    INPUT: df\n",
    "    \n",
    "    V1\tV2\t  V3\tV4\tV5\tV6\tV7\n",
    "    2\t30\t-1.1\t1\t1\t3\t2\n",
    "    3\t32\t-0.6\t2\t3\t2\t2\n",
    "    2\t30\t0.13\t1\t1\t4\t2\n",
    "    \n",
    "    OUTPUT: promotion\n",
    "    \n",
    "    array(['Yes', 'Yes', 'No'])\n",
    "    indicating the first two users would recieve the promotion and \n",
    "    the last should not.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will test your results, and provide you back some information \n",
    "# on how well your promotion_strategy will work in practice\n",
    "\n",
    "test_results(promotion_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = pd.read_csv('./Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
